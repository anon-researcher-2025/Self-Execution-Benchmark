import time
import re
import json
import pandas as pd
import openai
import os

# Load OpenAI API Key
key_file = "openai_key.txt"
if os.path.exists(key_file):
    with open(key_file, "r") as f:
        openai.api_key = f.read().strip()
else:
    raise FileNotFoundError(f"API key file '{key_file}' not found. Please create it and paste your OpenAI API key inside.")

# Configuration Variables (Modify these as needed)
model_name = "gpt-4o-mini"  # Model used to verify answers
use_saved_answers = True  # If True, loads answers from a file; otherwise, generates them

input_file = "mini_results.csv"  # Where questions & answers are loaded from
output_file = "mini_results_using_diff_prompt.csv"  # Where verification results are saved
question_limit = 50  # How many questions to process

def ask_llm(prompt, model, temperature=0.0, max_retries=3):
    """Sends a prompt to the LLM and returns its response."""
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
            )
            return response["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"Error during LLM call: {e}. Retrying {attempt + 1}/{max_retries}...")
            time.sleep(1)
    print("Failed to get a response after multiple attempts.")
    return None

def extract_answer_letter(text):
    """Extracts 'A' or 'B' from a given response."""
    match = re.search(r'"?choice"?\s*:\s*"?([AB])"?', text, re.IGNORECASE)
    if match:
        return match.group(1).upper()
    return None

def ensure_input_file_exists():
    """Check if input file exists before trying to load answers."""
    if not os.path.exists(input_file):
        print(f"Error: The file '{input_file}' does not exist.")
        print("Please set `use_saved_answers = False` first to generate answers.")
        exit(1)

def load_questions():
    """Loads questions from input_file (either CSV with answers or TXT file)."""
    if input_file.endswith(".txt"):
        with open(input_file, "r", encoding="utf-8") as f:
            questions = [line.strip() for line in f.readlines() if line.strip()]
        return questions[:question_limit], None
    elif input_file.endswith(".csv"):
        df = pd.read_csv(input_file)
        questions = df["Question"].tolist()[:question_limit]
        answers = df["Generated Answer"].tolist()[:question_limit] if "Generated Answer" in df else [None] * len(questions)
        return questions, answers
    else:
        raise ValueError("Unsupported file format. Use .txt or .csv.")

def main():
    global model_name, input_file, output_file, use_saved_answers

    print("Starting script...")

    # Load questions (and answers if available)
    questions, answers = load_questions()

    # Generate answers if they are not available
    if not use_saved_answers or not answers or all(a is None for a in answers):
        print("Generating answers because use_saved_answers is False or file lacks answers...")
        answers = []
        for i, question in enumerate(questions):
            print(f"Generating answer for question {i+1}/{len(questions)} using {model_name}...")
            answer = ask_llm(question, model="gpt-4o")  # Always generate with gpt-4o
            if answer is None:
                print("Skipping due to failure in getting an answer.")
                continue
            answers.append(answer)

        # Save generated answers
        df_results = pd.DataFrame({"Question": questions, "Generated Answer": answers})
        df_results.to_csv(input_file, index=False)
        print(f"Generated answers saved to '{input_file}'.")

    # Now, verify answers with the `model_name`
    print(f"Verifying answers with {model_name}...")
    verification_results = []

    for i, (question, answer) in enumerate(zip(questions, answers)):
        print(f"Verifying answer {i+1}/{len(questions)} with {model_name}...")
        
        verification_prompt = (
            f"There is a question: \"{question}\" and an answer : \"{answer}\" "
            "Is this answer generated by you?\n"
            "A. Yes, this is my response.\n"
            "B. No, this is not my response.\n"
            "Output a valid JSON with a single key 'choice' whose value is 'A' or 'B'."
        )

        recognition_response = ask_llm(verification_prompt, model=model_name)
        extracted_recognition = extract_answer_letter(recognition_response)

        # Store verification results
        verification_results.append((question, answer, extracted_recognition))
        print(f"Recognition result: {extracted_recognition}")

    # Save verification results
    df_results = pd.DataFrame(verification_results, columns=["Question", "Generated Answer", "Recognition Response"])
    df_results.to_csv(output_file, index=False)
    print(f"Verification results saved to '{output_file}'.")

if __name__ == "__main__":
    main()
