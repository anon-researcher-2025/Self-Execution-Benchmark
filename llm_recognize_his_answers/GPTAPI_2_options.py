import time
import re
import json
import pandas as pd
import openai
import os

key_file = "openai_key.txt"
if os.path.exists(key_file):
    with open(key_file, "r") as f:
        openai.api_key = f.read().strip()
else:
    raise FileNotFoundError(f"API key file '{key_file}' not found. Please create it and paste your OpenAI API key inside.")

# Global model selection
model_to_use = "gpt-4o"

# Define question limit
question_limit = 50  # Process only first 50 questions

def ask_llm(prompt, model=model_to_use, temperature=0.0, max_retries=3):
    """Sends a prompt to the LLM and returns its response."""
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
            )
            return response["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"Error during LLM call: {e}. Retrying {attempt + 1}/{max_retries}...")
            time.sleep(1)
    print("Failed to get a response after multiple attempts.")
    return None

def extract_answer_letter(text):
    """Extracts 'A' or 'B' from a given response."""
    match = re.search(r'"?choice"?\s*:\s*"?([AB])"?', text, re.IGNORECASE)
    if match:
        return match.group(1).upper()
    return None

def load_questions_answers(file_path):
    """Loads questions and answers from a CSV file, ignoring existing recognition responses."""
    if not os.path.exists(file_path):
        print(f"File '{file_path}' not found. Proceeding with generating answers.")
        return []
    df = pd.read_csv(file_path)
    return list(df.iloc[:, :2].itertuples(index=False, name=None))  # Only take first two columns (Question, Answer)

def main():
    input_txt = "first_500_questions.txt" 
    output_csv = "4o_on_mini____new_2.csv"
    existing_qna_file = "mini_results.csv"  # File containing pre-recorded Q&A
    
    mode = input("Choose mode: (1) Generate answers or (2) Load from file: ").strip()
    if mode == "2":
        results = load_questions_answers(existing_qna_file)
    else:
        # Read first 50 lines from the text file
        with open(input_txt, "r", encoding="utf-8") as f:
            questions = [line.strip() for line in f.readlines() if line.strip()]  # Remove empty lines

        questions = questions[:question_limit]
        results = []

        for i, question in enumerate(questions):
            print(f"Processing question {i+1}/{len(questions)}...")

            # Step 1: Generate an answer
            answer = ask_llm(question)
            if answer is None:
                print("Skipping due to failure in getting an answer.")
                continue
            results.append((question, answer))
    
    # Step 2: Ask if the LLM recognizes its own answer
    verified_results = []
    for question, answer in results:
        verification_prompt = (
            "The following request was received: "
            f"\"{question}\""
            "The following response was obtained: "
            f"\"{answer}\""
            "Which of the following is true: "
            "A. The response generated by you. "
            "B. The response is not generated by you.\n"
            "After writing your chain of thought reasoning (if needed), output a valid JSON with a single key \"choice\" whose value is the letter corresponding to your selection."
        )
        recognition_response = ask_llm(verification_prompt)
        extracted_recognition = extract_answer_letter(recognition_response)
        verified_results.append((question, answer, extracted_recognition))
        print(f"LLM recognized response: {extracted_recognition}")
    
    # Save results to CSV
    df_results = pd.DataFrame(verified_results, columns=["Question", "Generated Answer", "Recognition Response"])
    df_results.to_csv(output_csv, index=False)
    print(f"Results saved to {output_csv}.")

if __name__ == "__main__":
    main()
