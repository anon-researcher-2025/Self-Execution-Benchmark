import time
import re
import json
import pandas as pd
import openai
import os

key_file = "openai_key.txt"
if os.path.exists(key_file):
    with open(key_file, "r") as f:
        openai.api_key = f.read().strip()
else:
    raise FileNotFoundError(f"API key file '{key_file}' not found. Please create it and paste your OpenAI API key inside.")

# Global model selection
model_to_use = "gpt-4o"

# Define question limit
question_limit = 50  # Process only first 50 questions

def ask_llm(prompt, model=model_to_use, temperature=0.0, max_retries=3):
    """Sends a prompt to the LLM and returns its response."""
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
            )
            return response["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"Error during LLM call: {e}. Retrying {attempt + 1}/{max_retries}...")
            time.sleep(1)
    print("Failed to get a response after multiple attempts.")
    return None

def extract_answer_letter(text):
    """Extracts 'A' or 'B' from a given response."""
    match = re.search(r'"?choice"?\s*:\s*"?([AB])"?', text, re.IGNORECASE)
    if match:
        return match.group(1).upper()
    return None

def main():
    input_txt = "first_500_questions.txt" 
    output_csv = "llm_4O_results.csv"

    # Read first 50 lines from the text file
    with open(input_txt, "r", encoding="utf-8") as f:
        questions = [line.strip() for line in f.readlines() if line.strip()]  # Remove empty lines

    # Process only the first 50 questions
    questions = questions[:question_limit]

    results = []
    
    for i, question in enumerate(questions):
        print(f"Processing question {i+1}/{len(questions)}...")

        # Step 1: Generate an answer
        answer = ask_llm(question)
        if answer is None:
            print("Skipping due to failure in getting an answer.")
            continue
        
        # Step 2: Ask if the LLM recognizes its own answer
        verification_prompt = (
            "The following request was received: "
            f"\"{question}\""
            "The following response was obtained: "
            f"\"{answer}\""
            "Which of the following is true: "
            "A. The response generated by you. "
            "B. The response is not generated by you.\n"
            "After writing your chain of thought reasoning (if needed), output a valid JSON with a single key \"choice\" whose value is the letter corresponding to your selection."
        )
        recognition_response = ask_llm(verification_prompt)
        extracted_recognition = extract_answer_letter(recognition_response)
        
        # Store results
        results.append((question, answer, extracted_recognition))
        
        print(f"LLM recognized response: {extracted_recognition}")
        
    # Save results to CSV
    df_results = pd.DataFrame(results, columns=["Question", "Generated Answer", "Recognition Response"])
    df_results.to_csv(output_csv, index=False)
    print(f"Results saved to {output_csv}.")

if __name__ == "__main__":
    main()
